# ğŸ’¬ **Sistema de Troca de Mensagens Bulletin Board**

## VisÃ£o Geral do Projeto

Este projeto consiste no desenvolvimento de uma versÃ£o simplificada de um sistema de troca de mensagens instantÃ¢neas.

O objetivo principal Ã© aplicar conceitos de **Sistemas DistribuÃ­dos** para construir um serviÃ§o que permita a comunicaÃ§Ã£o em tempo real e a persistÃªncia de dados.

## ğŸ¯ Funcionalidades Principais

O sistema implementa as seguintes interaÃ§Ãµes bÃ¡sicas entre usuÃ¡rios e o serviÃ§o:

* **Mensagens Privadas (P2P):** Troca de mensagens diretas entre dois usuÃ¡rios.
* **Canais PÃºblicos:** Postagem e visualizaÃ§Ã£o de mensagens em canais de discussÃ£o abertos.
* **PersistÃªncia de Dados:** Todas as interaÃ§Ãµes (mensagens privadas e postagens em canais) sÃ£o armazenadas em disco, permitindo que os usuÃ¡rios recuperem o histÃ³rico de mensagens anteriores.

---

## ğŸ‘©â€ğŸ’» Cliente

### Mensagens Enviadas
- `login`: `{ service: "login", data: { user, timestamp, clock } }`
- `users`: `{ service: "users", data: { timestamp, clock } }`
- `channel`: `{ service: "channel", data: { channel, timestamp, clock } }`
- `channels`: `{ service: "channels", data: { timestamp, clock } }`
- `publish`: `{ service: "publish", data: { user, channel, message, timestamp, clock } }`
- `message`: `{ service: "message", data: { src, dst, message, timestamp, clock } }`

### Mensagens Recebidas
- Respostas REP do servidor com `status`, `timestamp`, `clock` e, quando aplicÃ¡vel, campos de erro (`description`/`message`).
- PublicaÃ§Ãµes via proxy (`proxy:5558`) para:
  - TÃ³pico com o nome do usuÃ¡rio (mensagens privadas).
  - TÃ³picos de canais inscritos (mensagens em canais).

O cliente interativo (`cliente.py`) e o cliente automÃ¡tico (`cliente_automatico.py`) compartilham o mesmo protocolo MessagePack, incrementando o relÃ³gio lÃ³gico antes de cada envio e atualizando com base nas mensagens recebidas.

---

## ğŸ› ï¸ Tecnologias e Arquitetura

Este projeto seguiu um conjunto de padronizaÃ§Ãµes para garantir a interoperabilidade e a testabilidade, e integrou escolhas livres de tecnologia para demonstrar proficiÃªncia em mÃºltiplas linguagens e armazenamento de dados.

### ğŸ³ Arquitetura em Containers

O ambiente distribuÃ­do roda inteiramente em containers Docker orquestrados via `docker compose`. A topologia foi organizada para refletir o enunciado do projeto, com trÃªs instÃ¢ncias de servidor compartilhando o mesmo cÃ³digo, dois clientes automÃ¡ticos (bots) gerando carga, um cliente interativo, um broker, um proxy Pub/Sub e o servidor de referÃªncia que provÃª eleiÃ§Ã£o e sincronizaÃ§Ã£o.

```mermaid
graph LR
    ref <--req/rep--> server_1
    ref <--req/rep--> server_2
    ref <--req/rep--> server_3

    server_1 <--req/rep--> broker
    server_2 <--req/rep--> broker
    server_3 <--req/rep--> broker

    server_1 --pub--> proxy
    server_2 --pub--> proxy
    server_3 --pub--> proxy

    broker <--req/rep--> client
    broker <--req/rep--> bot_1
    broker <--req/rep--> bot_2

    proxy --sub--> client
    proxy --sub--> bot_1
    proxy --sub--> bot_2
```

No arquivo `docker-compose.yml` os serviÃ§os com mÃºltiplas instÃ¢ncias (`servidor` e `cliente_automatico`) sÃ£o declarados com `deploy.replicas`, garantindo que todas as rÃ©plicas utilizem exatamente o mesmo cÃ³digo e configuraÃ§Ã£o.

---

## ğŸ–¥ï¸ Servidor

### Mensagens Recebidas
- RequisiÃ§Ãµes REQ/REP encaminhadas pelo broker (`broker:5556`) com `service`/`data` serializados em MessagePack.
- Chamadas ao servidor de referÃªncia (`rank`, `list`, `heartbeat`) para eleiÃ§Ã£o, monitoramento e clock lÃ³gico.
- PublicaÃ§Ãµes via proxy (`proxy:5558`) nos tÃ³picos `servers` (anÃºncio de coordenador) e `replication` (eventos de dados).
- RequisiÃ§Ãµes diretas de outros servidores (`election`, `clock`) na porta 5560.

### Mensagens Enviadas
- Respostas REP aos clientes preservando o `service` original e preenchendo `status`, `timestamp`, `clock` e mensagens de erro quando necessÃ¡rio.
- PublicaÃ§Ãµes em canais e mensagens privadas via proxy (`proxy:5557`), usando o nome do canal ou do destinatÃ¡rio como tÃ³pico.
- Eventos de replicaÃ§Ã£o no tÃ³pico `replication`, incluindo `operation`, `operationData`, `serverName`, `timestamp` e `clock`.
- Broadcasts no tÃ³pico `servers` anunciando novos coordenadores.
- Replies `rank/list/heartbeat` para o servidor de referÃªncia e `election/clock` para outros servidores.

Todas as operaÃ§Ãµes mutÃ¡veis sÃ£o persistidas em `data/` (arquivos JSON) e replicadas para garantir consistÃªncia eventual entre as rÃ©plicas.

### ğŸŒ PadronizaÃ§Ãµes (Requisitos do Enunciado)

| Componente | Detalhe |
| :--- | :--- |
| **ComunicaÃ§Ã£o** | Uso da biblioteca **ZeroMQ (Ã˜MQ)** para a troca de mensagens entre os atores do sistema (clientes e servidor). |
| **Formato de Mensagem** | As mensagens seguem um **padrÃ£o definido** (descrito na documentaÃ§Ã£o tÃ©cnica) para garantir a interconexÃ£o. |
| **Ambiente de Teste** | UtilizaÃ§Ã£o de **Containers** (`Docker`) para encapsular e executar as instÃ¢ncias de cada ator (cliente e servidor), facilitando os testes de ambiente distribuÃ­do. |

### Como Executar

**(Aqui vocÃª descreveria, de forma resumida, os passos para clonar, construir as imagens Docker e iniciar o servidor e clientes.)**

1.  Clone o repositÃ³rio: `git clone https://github.com/gbalbuquerque/BulletinBoard.git`
2.  Navegue atÃ© o diretÃ³rio do projeto
3.  Construa as imagens dos containers (se aplicÃ¡vel): `docker compose build`
4.  Inicie o sistema: `docker compose up`

---

## ğŸ”„ ConsistÃªncia e ReplicaÃ§Ã£o de Dados

### Problema Identificado

O sistema utiliza um broker com balanceamento de carga do tipo **round-robin**, onde as requisiÃ§Ãµes dos clientes sÃ£o distribuÃ­das sequencialmente entre os servidores disponÃ­veis. Isso resulta em uma situaÃ§Ã£o onde:

- Cada servidor recebe apenas uma **parte** das mensagens trocadas no sistema
- Se um servidor falhar, **perde-se o histÃ³rico** de mensagens que ele processou
- Quando um cliente solicita o histÃ³rico de mensagens, recebe apenas os dados armazenados no servidor que atendeu a requisiÃ§Ã£o, nÃ£o o histÃ³rico completo

### SoluÃ§Ã£o Implementada: ReplicaÃ§Ã£o Multi-Master com Pub/Sub

Foi implementado um sistema de **replicaÃ§Ã£o Multi-Master** utilizando o padrÃ£o **Publisher-Subscriber** jÃ¡ existente no projeto. Este mÃ©todo foi escolhido porque:

1. **Alta Disponibilidade**: Todos os servidores podem processar requisiÃ§Ãµes e manter dados atualizados
2. **TolerÃ¢ncia a Falhas**: Se um servidor falhar, os outros continuam com dados completos
3. **Desempenho**: NÃ£o hÃ¡ gargalo de um servidor primÃ¡rio Ãºnico
4. **Simplicidade**: Aproveita a infraestrutura Pub/Sub jÃ¡ implementada

### Funcionamento

#### 1. Processamento de OperaÃ§Ãµes

Quando um servidor processa uma operaÃ§Ã£o que **modifica dados** (login, criaÃ§Ã£o de canal, publicaÃ§Ã£o em canal, ou mensagem direta), ele:

1. **Executa a operaÃ§Ã£o localmente** (salva no banco de dados local)
2. **Publica uma mensagem de replicaÃ§Ã£o** no tÃ³pico `"replication"` via Pub/Sub
3. **Retorna resposta ao cliente** normalmente

#### 2. Recebimento de ReplicaÃ§Ãµes

Cada servidor possui um **subscriber** inscrito no tÃ³pico `"replication"` que:

1. **Recebe mensagens de replicaÃ§Ã£o** de outros servidores
2. **Verifica se a mensagem nÃ£o Ã© do prÃ³prio servidor** (evita loops)
3. **Aplica a mesma operaÃ§Ã£o localmente** sem replicar novamente (flag `isReplicating`)
4. **Atualiza o relÃ³gio lÃ³gico** conforme a mensagem recebida

#### 3. OperaÃ§Ãµes Replicadas

As seguintes operaÃ§Ãµes sÃ£o replicadas entre servidores:

- **`login`**: Cadastro de novos usuÃ¡rios
- **`channel`**: CriaÃ§Ã£o de novos canais
- **`publish`**: PublicaÃ§Ãµes em canais
- **`message`**: Mensagens diretas entre usuÃ¡rios

### Formato das Mensagens de ReplicaÃ§Ã£o

As mensagens de replicaÃ§Ã£o seguem o formato MessagePack e sÃ£o publicadas no tÃ³pico `"replication"`:

```json
{
  "service": "replication",
  "data": {
    "operation": "login" | "channel" | "publish" | "message",
    "operationData": {
      // Dados especÃ­ficos da operaÃ§Ã£o
      // Para login: { "user": "...", "timestamp": ... }
      // Para channel: { "channel": "...", "timestamp": ... }
      // Para publish: { "channel": "...", "user": "...", "message": "...", "timestamp": ... }
      // Para message: { "src": "...", "dst": "...", "message": "...", "timestamp": ... }
    },
    "serverName": "nome_do_servidor_origem",
    "timestamp": 1234567890,
    "clock": 42
  }
}
```

### ModificaÃ§Ãµes no MÃ©todo Tradicional

O mÃ©todo Multi-Master tradicional foi adaptado para este projeto com as seguintes modificaÃ§Ãµes:

1. **Uso de Pub/Sub ao invÃ©s de comunicaÃ§Ã£o direta**: Aproveita a infraestrutura existente e simplifica a implementaÃ§Ã£o
2. **Flag de replicaÃ§Ã£o**: Evita loops infinitos onde um servidor replica sua prÃ³pria replicaÃ§Ã£o
3. **IdentificaÃ§Ã£o do servidor origem**: Cada mensagem inclui o nome do servidor que originou a operaÃ§Ã£o, permitindo que servidores ignorem suas prÃ³prias replicaÃ§Ãµes
4. **SincronizaÃ§Ã£o de relÃ³gio lÃ³gico**: As mensagens de replicaÃ§Ã£o incluem o valor do relÃ³gio lÃ³gico para manter a consistÃªncia temporal

### Diagrama de SequÃªncia

```mermaid
sequenceDiagram
    Cliente->>Servidor1: REQ: publish (canal, mensagem)
    Servidor1->>Servidor1: Salva localmente
    Servidor1->>Pub/Sub: PUB: replication (publish)
    Servidor1->>Cliente: REP: OK
    
    Pub/Sub->>Servidor2: PUB: replication (publish)
    Servidor2->>Servidor2: Aplica replicaÃ§Ã£o (salva)
    
    Pub/Sub->>Servidor3: PUB: replication (publish)
    Servidor3->>Servidor3: Aplica replicaÃ§Ã£o (salva)
```

### Garantias de ConsistÃªncia

- **ConsistÃªncia Eventual**: Todos os servidores eventualmente terÃ£o os mesmos dados
- **IdempotÃªncia**: Aplicar a mesma replicaÃ§Ã£o mÃºltiplas vezes nÃ£o causa problemas (verificaÃ§Ãµes de existÃªncia)
- **Ordem Parcial**: O relÃ³gio lÃ³gico garante ordem parcial das operaÃ§Ãµes

### LimitaÃ§Ãµes e ConsideraÃ§Ãµes

1. **ConsistÃªncia Eventual**: HÃ¡ um pequeno delay entre a operaÃ§Ã£o original e sua replicaÃ§Ã£o
2. **Sem ResoluÃ§Ã£o de Conflitos**: NÃ£o hÃ¡ mecanismo para resolver conflitos se dois servidores modificarem os mesmos dados simultaneamente (nÃ£o Ã© necessÃ¡rio neste projeto devido Ã  natureza das operaÃ§Ãµes)
3. **Perda de Mensagens**: Se um servidor estiver offline durante uma replicaÃ§Ã£o, ele nÃ£o receberÃ¡ aquela mensagem especÃ­fica (poderia ser implementado um mecanismo de sincronizaÃ§Ã£o inicial)

